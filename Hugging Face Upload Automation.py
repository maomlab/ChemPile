## Hugging Face Upload Automation

import pandas as pd
import numpy as np
import urllib.request
import webbrowser
from tqdm import tqdm

######################################
# Part 1- Dataset Information 

# You can directly choose the categories below on Hugging Face
#  License, Task_categories, Language, Tags, Pretty_name, and Size_Categories
# Codes for those categories would be autogenerated by Hugging Face

# Inputs needed to be typed
dataset_name = input('Write a dataset name: ')
pretty_name = input('Write a dataset pretty name: ')
dataset_summary = input('Write a dataset summary: ')
config_names = input('Write the file name(s) separated by comma: ')
DOI = input('Write the DOI of the paper: ')

# Website that converts DOI to BibTeX citation
# Copy and paste the BibTeX citation that you got from BibTeX website
url = f"https://www.doi2bib.org/bib/{DOI}"
webbrowser.open_new_tab(url)

# It's time to print all the information!
print (f'''pretty_name: {pretty_name}
dataset_summary: >- \n {dataset_summary}
citation: >- \n COPY AND PASTE WHAT YOU GOT FROM THE BIBTEX WEBSITE 
''')

config_names_list = [name.strip() for name in config_names.split(',')]
print("config_names:")
for name in config_names_list:
    print(f"- {name}")

print("configs:", end='')
for name in config_names_list:
    print(f'''
- config_name: {name}
  data_files:
  - split: test
    path: {name}/test.csv
  - split: train
    path: {name}/train.csv''', end='')

train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

print(f'''
dataset_info:
- config_name: {name}
  features:
''', end='')
for column_name, dtype in zip(test.columns, test.dtypes):
    dtype_str = "string" if dtype == "object" else dtype
    print(f"    - name: \"{column_name}\"")
    print(f"      dtype: {dtype_str}")

train_memory_usage = train.memory_usage(index=True).sum()
test_memory_usage = test.memory_usage(index=True).sum()
train_num_examples = len(train)
test_num_examples = len(test)

print(f'''  splits:
    - name: train
      num_bytes: {train_memory_usage}
      num_examples: {train_num_examples}
    - name: test
      num_bytes: {test_memory_usage}
      num_examples: {test_num_examples}
''')


######################################
# Part 2- Dataset Usage

print(f'''# {pretty_name}
\n\n## Quickstart Usage
\n### Load a dataset in python
Each subset can be loaded into python using the Huggingface [datasets](https://huggingface.co/docs/datasets/index) library. 
First, from the command line install the `datasets` library 
\n     $ pip install datasets
\nthen, from within python load the datasets library
\n    >>> import datasets
\nand load one of the `{dataset_name}` datasets, e.g., 
\n\n and inspecting the loaded dataset                          
\n''')

print('''### Use a dataset to train a model
One way to use the dataset is through the [MolFlux](https://exscientia.github.io/molflux/) package developed by Exscientia.
First, from the command line, install `MolFlux` library with `catboost` and `rdkit` support
\n    pip install 'molflux[catboost,rdkit]'            
\nthen load, featurize, split, fit, and evaluate the a catboost model
\n    import json
    from datasets import load_dataset
    from molflux.datasets import featurise_dataset
    from molflux.features import load_from_dicts as load_representations_from_dicts
    from molflux.splits import load_from_dict as load_split_from_dict
    from molflux.modelzoo import load_from_dict as load_model_from_dict
    from molflux.metrics import load_suite
\n    split_dataset = load_dataset('maomlab/B3DB', name = 'B3DB_classification')
\n    split_featurised_dataset = featurise_dataset(
      split_dataset,
      column = "SMILES",
      representations = load_representations_from_dicts([{"name": "morgan"}, {"name": "maccs_rdkit"}]))
 \n    model = load_model_from_dict({
        "name": "cat_boost_classifier",
        "config": {
            "x_features": ['SMILES::morgan', 'SMILES::maccs_rdkit'],
            "y_features": ['BBB+/BBB-']}})
\n    model.train(split_featurised_dataset["train"])
    preds = model.predict(split_featurised_dataset["test"])
\n    classification_suite = load_suite("classification")
\n    scores = classification_suite.compute(
        references=split_featurised_dataset["test"]['BBB+/BBB-'],
        predictions=preds["cat_boost_classifier::BBB+/BBB-"])
''')                                                       
      
print("    >>> B3DB_classification = datasets.load_dataset(\"maomlab/B3DB\", name = \"B3DB_classification\")")
with tqdm(total=100, desc="Downloading readme", unit='%', ncols=100, position=0, leave=True) as pbar:
    pbar.update(100)
with tqdm(total=train_memory_usage, desc="Downloading data", unit='%', ncols=100, position=0, leave=True) as pbar:
    pbar.update(train_memory_usage)
with tqdm(total=test_memory_usage, desc="Downloading data", unit='%', ncols=100, position=0, leave=True) as pbar:
    pbar.update(test_memory_usage)
with tqdm(total=train_num_examples, desc="Generating test split", unit=' examples', ncols=100, position=0, leave=True) as pbar:
    pbar.update(train_num_examples)
with tqdm(total=test_num_examples, desc="Generating train split", unit=' examples', ncols=100, position=0, leave=True) as pbar:
    pbar.update(test_num_examples)

print(f'''## About the {dataset_name}
\n### Features of {dataset_name}      
\n\n### Data splits
The original {dataset_name} dataset does not define splits, so here we have used the `Realistic Split` method 
described in [(Martin et al., 2018)](https://doi.org/10.1021/acs.jcim.7b00166).
\n###Citation \n
''')

